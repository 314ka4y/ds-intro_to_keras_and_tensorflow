{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-keras-and-tensorflow\" data-toc-modified-id=\"Introduction-to-keras-and-tensorflow-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to <code>keras</code> and <code>tensorflow</code></a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#How-Traning-Neural-Networks-Work\" data-toc-modified-id=\"How-Traning-Neural-Networks-Work-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>How Traning Neural Networks Work</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-Propagation\" data-toc-modified-id=\"Forward-Propagation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Forward Propagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-Propagation-Implemented-from-Scratch\" data-toc-modified-id=\"Forward-Propagation-Implemented-from-Scratch-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Forward Propagation Implemented from Scratch</a></span></li></ul></li><li><span><a href=\"#Backpropagation-(Backprop)\" data-toc-modified-id=\"Backpropagation-(Backprop)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Backpropagation (Backprop)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-Approximation\" data-toc-modified-id=\"Function-Approximation-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Function Approximation</a></span></li><li><span><a href=\"#Loss-Function\" data-toc-modified-id=\"Loss-Function-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Loss Function</a></span></li></ul></li><li><span><a href=\"#Adjusting-Weights-with-Gradient-Descent\" data-toc-modified-id=\"Adjusting-Weights-with-Gradient-Descent-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Adjusting Weights with Gradient Descent</a></span></li></ul></li><li><span><a href=\"#♫-Better,-Faster,-Stronger-♫\" data-toc-modified-id=\"♫-Better,-Faster,-Stronger-♫-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>♫ Better, Faster, Stronger ♫</a></span><ul class=\"toc-item\"><li><span><a href=\"#Some-useful-resources-on-optimization\" data-toc-modified-id=\"Some-useful-resources-on-optimization-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Some useful resources on optimization</a></span></li><li><span><a href=\"#A-Need-for-Optimization:-Vanishing-Gradient-Problem\" data-toc-modified-id=\"A-Need-for-Optimization:-Vanishing-Gradient-Problem-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>A Need for Optimization: Vanishing Gradient Problem</a></span></li><li><span><a href=\"#Optimizers\" data-toc-modified-id=\"Optimizers-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Optimizers</a></span></li><li><span><a href=\"#Batch-Gradient-Descent\" data-toc-modified-id=\"Batch-Gradient-Descent-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Batch Gradient Descent</a></span></li><li><span><a href=\"#Normalization-(Batch-Normalization)\" data-toc-modified-id=\"Normalization-(Batch-Normalization)-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Normalization (Batch Normalization)</a></span></li><li><span><a href=\"#Stochastic-Gradient-Descent\" data-toc-modified-id=\"Stochastic-Gradient-Descent-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Stochastic Gradient Descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Steps\" data-toc-modified-id=\"Steps-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>Steps</a></span></li><li><span><a href=\"#Mini-Batch-Gradient-Descent\" data-toc-modified-id=\"Mini-Batch-Gradient-Descent-4.6.2\"><span class=\"toc-item-num\">4.6.2&nbsp;&nbsp;</span>Mini-Batch Gradient Descent</a></span></li></ul></li><li><span><a href=\"#Activation-Functions\" data-toc-modified-id=\"Activation-Functions-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;</span>Activation Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recall-What-an-Activation-Function-Does\" data-toc-modified-id=\"Recall-What-an-Activation-Function-Does-4.7.1\"><span class=\"toc-item-num\">4.7.1&nbsp;&nbsp;</span>Recall What an Activation Function Does</a></span></li><li><span><a href=\"#Using-the-Proper-Activation\" data-toc-modified-id=\"Using-the-Proper-Activation-4.7.2\"><span class=\"toc-item-num\">4.7.2&nbsp;&nbsp;</span>Using the Proper Activation</a></span></li></ul></li><li><span><a href=\"#Optimizers\" data-toc-modified-id=\"Optimizers-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;</span>Optimizers</a></span></li></ul></li><li><span><a href=\"#Tensorflow-and-Keras\" data-toc-modified-id=\"Tensorflow-and-Keras-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Tensorflow and Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras-is-an-API\" data-toc-modified-id=\"Keras-is-an-API-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Keras is an API</a></span></li><li><span><a href=\"#Building-a-Binary-Classifier-NN\" data-toc-modified-id=\"Building-a-Binary-Classifier-NN-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Building a Binary Classifier NN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialize-a-Linear-Stack-of-Layers\" data-toc-modified-id=\"Initialize-a-Linear-Stack-of-Layers-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Initialize a Linear Stack of Layers</a></span></li><li><span><a href=\"#Add-Densely-Connected-Layers\" data-toc-modified-id=\"Add-Densely-Connected-Layers-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Add Densely Connected Layers</a></span></li><li><span><a href=\"#Compile-the-Model\" data-toc-modified-id=\"Compile-the-Model-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Compile the Model</a></span></li><li><span><a href=\"#Fit-the-Model\" data-toc-modified-id=\"Fit-the-Model-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Fit the Model</a></span></li></ul></li><li><span><a href=\"#Appendix:-More-on-Tensorflow-Vs.-Keras\" data-toc-modified-id=\"Appendix:-More-on-Tensorflow-Vs.-Keras-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Appendix: More on Tensorflow Vs. Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Let's-start-with-tensors\" data-toc-modified-id=\"Let's-start-with-tensors-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Let's start with tensors</a></span></li><li><span><a href=\"#TensorFlow-manages-the-flow-of-matrix-math\" data-toc-modified-id=\"TensorFlow-manages-the-flow-of-matrix-math-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>TensorFlow manages the flow of matrix math</a></span></li></ul></li><li><span><a href=\"#TensorFlow-has-more-levers-and-buttons,-but-Keras-is-more-user-friendly\" data-toc-modified-id=\"TensorFlow-has-more-levers-and-buttons,-but-Keras-is-more-user-friendly-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>TensorFlow has more levers and buttons, but Keras is more user-friendly</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras,-an-API-with-an-intentional-UX\" data-toc-modified-id=\"Keras,-an-API-with-an-intentional-UX-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Keras, an API with an intentional UX</a></span></li><li><span><a href=\"#A-few-comparisons\" data-toc-modified-id=\"A-few-comparisons-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>A few comparisons</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction to `keras` and `tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_digits, load_sample_images\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "    \n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Describe the concept of backpropagation\n",
    "- Explain the use of gradient descent in neural networks\n",
    "- Use `keras` to code up a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# How Traning Neural Networks Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![backprop](img/ff-bb.gif)\n",
    "\n",
    "The graphic above can be a bit frustrating since it moves fast, but follow the progress as so:\n",
    "\n",
    "Forward propagation with the **blue** tinted arrows computes the output of each layer: i.e. a summation and activation.\n",
    "\n",
    "Backprop calculates the partial derivative (**green** circles) for each weight (**brown** line) and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we saw from last time, forward propagation. Is making a prediction with the neural network after the weights are \"frozen\" (unchanging). \n",
    "\n",
    "We feed in each data point through the neural network to make a prediction. This is usually relatively quick and the real magic comes in when we try to find ways to adjust our different weights to get a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Forward Propagation Implemented from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's consider some images that we'll use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "flat_image = np.array(digits.data[0]).reshape(digits.data[0].shape[0], -1)\n",
    "eight_by_eight_image = digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eight_by_eight_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(eight_by_eight_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data,\n",
    "                                                    digits.target,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Z is the input from our collector, the sum of the weights\n",
    "# multiplied by the features and the bias\n",
    "\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Input the sum of our weights times the pixel intensities, plus the bias\n",
    "    Output a number between 0 and 1.\n",
    "    \n",
    "    '''\n",
    "    return 1/(1+np.e**(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Simulation from last time\n",
    "\n",
    "w_1 = np.random.normal(0, 0.1, (X_train.shape[1], 1))\n",
    "w_1.shape\n",
    "\n",
    "b_1 = 0\n",
    "\n",
    "z_1 = X_train.dot(w_1) + b_1\n",
    "z_1\n",
    "\n",
    "output = sigmoid(z_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = output > .5\n",
    "y_hat = y_pred.astype(int)\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Backpropagation (Backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After a certain number of data points have been passed through the model, the weights will be *updated* with an eye toward optimizing our loss function. (Thinking back to biological neurons, this is like revising their activation potentials.) Typically, this is  done  by using some version of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![bprop](img/BackProp_web.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Neural networks are much like computational graphs](https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9).\n",
    "\n",
    "And computational graphs can be used [to approximate *any* function](http://neuralnetworksanddeeplearning.com/chap4.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we're thinking of networks, then, as function approximators, of course we'll want to know how good the approximation is. And so once again we have the idea of a [loss function](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html), which is of course what licenses our thinking of networks as models in the first place.\n",
    "\n",
    "Many loss functions are available. Your choice will depend in part on whether you're doing a regression or classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Regression:\n",
    "\n",
    "- mean / median absolute error\n",
    "- mean / median squared error\n",
    "- [Huber loss](https://en.wikipedia.org/wiki/Huber_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Classification:\n",
    "\n",
    "- Crossentropy\n",
    "- [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss)\n",
    "- [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The loss function tells us how well our model performed by comparing the predictions to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we train our models with `keras`, we will watch the loss function's progress across epochs.  A decreasing loss function will show us that our model is **improving**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The loss function is associated with the nature of our output. In logistic regression, our output was binary, so our loss function was the negative loglikelihood, aka **cross-entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\Large -\\ loglikelihood = -\\frac{1}{m} * \\sum\\limits_{i=1}^m y_i\\log{p_i} + (1-y_i)\\log(1-p_i) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train %2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train*np.log(output) + (1-y_train) * np.log(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "neg_ll = -1/len(y_train)*np.sum(y_train*np.log(output) + (1-y_train) * np.log(1-output))\n",
    "neg_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For continuous variables, the loss function we have relied on is [MSE or MAE](http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/).\n",
    "\n",
    "Good [resource](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) on backpropogation with RMSE loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is a good summary of different [loss functions]( https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Adjusting Weights with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We not only use the the loss function to see how our model is improving; we also use it to update our parameters. The gradient of the loss function is calculated in relation to each parameter of our neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a deep dive into the fitting process, reference Chapter 11 in [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Gradient descent can be performed in several different ways.  Unlike the `sklearn` implementation of linear regression, which finds the minimum of the loss with a closed form solution, neural networks move down the gradient **incrementally**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When **all examples** from a training set have passed through the network and weights are adjusted with backpropagation, we say an ***epoch*** has been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ♫ Better, Faster, Stronger ♫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can optimize our model to run faster and better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes this optimization feels like black magic over a science...\n",
    "\n",
    "![https://xkcd.com/1838/](https://imgs.xkcd.com/comics/machine_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Some useful resources on optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Keras documentation & discussion: https://keras.io/optimizers/\n",
    "- Excellent blog post on optimizations: http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A Need for Optimization: Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When our gradients are too small we can run into a few issues when training our model (adjusting weights and biases):\n",
    "- Slow training\n",
    "- Local minimum problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Different types of gradient descent update the parameters at different times in different ways.\n",
    "\n",
    "> https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The gradient is calculated across all values.  We can find the direction of the gradient, and proceed directly towards the minimum.\n",
    "\n",
    "The weights are updated with regard to the cost at the **end of an epoch** after all training elements have passed through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Normalization (Batch Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> https://keras.io/layers/normalization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This allows speedier training so no one feature will overpower the direction down the hill\n",
    "\n",
    "Note if you use something like batch normalization, training speed can slow but converging to the minimal cost will tend to happen sooner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "SGD updates the weights after each training **example**. SGD requires fewer epochs to achieve quality coefficients. This speeds up gradient descent [significantly](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Instead of taking a long time through the whole process with dataset (careful step), go through part of the dataset quicker (quick, \"drunken\" steps). Note that this means it will also take less space in memory.\n",
    "\n",
    "\"Stochastic\" == Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/sgd_visualization.png)\n",
    "\n",
    "> Note: You likely will want to combine this momentum and other tweaks like learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Take a random set (batch)\n",
    "2. Feed forward batch through model\n",
    "3. Calculate error (loss) from batch\n",
    "4. Adjust weights via backpropogagtion\n",
    "5. Repeat with all points/batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In mini-batch, we pass a batch, calculate the gradient, update the params, then proceed to the next batch. It combines the advantages of batch and stochastic gradient descent: it is faster than SGD since the updates are not made with each point, and more computationally efficient than batch, since we don't have to hold all training examples in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Good comparison of types of Gradient Descent and batch size](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> We can modify what activation we use to help with the gradient during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Recall What an Activation Function Does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/relu_bouncer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using the Proper Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A simple way to change the gradient, is change the activation function that produces different gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/activations_ranked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In general:\n",
    "\n",
    "* Sigmoid should almost never be used anymore for an activation \n",
    "* ReLU is a good balance of speed and performance (most systems have optimizations)\n",
    "* You may find other variants like leaky ReLU to give better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One of the levers we can tweak are the optimizers which control how the weights and biases are updated.\n",
    "\n",
    "For stochastic gradient descent, the weights are updated with a **constant** learning rate (alpha) after every record.  If we specify a batch size, the constant learning rate is multiplied by the gradient across the batch. \n",
    "\n",
    "Other optimizers, such as **Adam** (\"Adaptive Moment Estimation\"), update the weights in different ways. For Adam,\n",
    "> A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds. See [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then the optimizer multiplies a **learning rate** ($\\eta$) to each partial derivative to calculate a new weight which will be applied to the next batch that passes through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wait a second, what is that warning? \n",
    "`Using TensorFlow backend.`\n",
    "\n",
    "<img align =left src=\"img/keras.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Keras is an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It can be layered on top of many different back-end processing systems.\n",
    "\n",
    "![kerasback](img/keras_tf_theano.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While each of these systems has its own coding methods, `keras` abstracts from that in the streamlined Pythonic manner we are used to seeing in other Python modeling libraries.\n",
    "\n",
    "Keras development is backed primarily by Google, and the Keras API comes packaged in TensorFlow as tf.keras. Additionally, Microsoft maintains the CNTK Keras backend. Amazon AWS is maintaining the Keras fork with MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML).\n",
    "\n",
    "Theano has been discontinued.  The last release was 2017, but can still be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use TensorFlow, as it is the most popular. TensorFlow became the most used Keras backend, and  eventually integrated Keras in via its `tf.keras` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Building a Binary Classifier NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will start with a binary classification, and predict whether the number will be even or odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_binary = y % 2\n",
    "y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Initialize a Linear Stack of Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Add Densely Connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(12, activation='relu', input_dim=64))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next step is new: After building the model we'll now **compile** it, which is a matter of yoking together the architecture with:\n",
    "- the optimizer we want to use,\n",
    "- the loss function we want to use, and\n",
    "- the metrics we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we're ready to **fit** it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X, y_binary, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Appendix: More on Tensorflow Vs. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Let's start with tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tensors are multidimensional matrices.\n",
    "\n",
    "![tensor](img/tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### TensorFlow manages the flow of matrix math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That makes neural network processing possible.\n",
    "\n",
    "![cat](img/cat-tensors.gif)\n",
    "\n",
    "For our numbers dataset, our tensors from the `sklearn` dataset were originally tensors of the shape 8x8, i.e. 64-bit pictures. Remember, that was with black and white images.\n",
    "\n",
    "For image processing, we are often dealing with color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image = load_sample_images()['images'][0]\n",
    "\n",
    "imgplot = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What do the dimensions of our image above represent?\n",
    "\n",
    "Tensors with higher numbers of dimensions have a higher **rank**.\n",
    "\n",
    "A matrix with rows and columns only, like the black and white numbers, has **rank 2**.\n",
    "\n",
    "A matrix with a third dimension, like the color pictures above, has **rank 3**.\n",
    "\n",
    "When we flatten an image by stacking the rows in a column, we are decreasing the rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flat_image = image.reshape(-1, 1)\n",
    "\n",
    "flat_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "427*640*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## TensorFlow has more levers and buttons, but Keras is more user-friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Coding directly in **Tensorflow** allows you to tweak more parameters to optimize performance. The **Keras** wrapper makes the code more accessible for developers prototyping models.\n",
    "\n",
    "![levers](img/levers.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Keras, an API with an intentional UX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Deliberately design end-to-end user workflows\n",
    "- Reduce cognitive load for your users\n",
    "- Provide helpful feedback to your users\n",
    "\n",
    "[full article here](https://blog.keras.io/user-experience-design-for-apis.html)<br>\n",
    "[full list of why to use Keras](https://keras.io/why-use-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### A few comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While you **can leverage both**, here are a few comparisons.\n",
    "\n",
    "| Comparison | Keras | Tensorflow|\n",
    "|------------|-------|-----------|\n",
    "| **Level of API** | high-level API | High and low-level APIs |\n",
    "| **Speed** |  can *seem* slower |  is a bit faster |\n",
    "| **Language architecture** | simple architecture, more readable and concise | straight tensorflow is a bit more complex |\n",
    "| **Debugging** | less frequent need to debug | difficult to debug |\n",
    "| **Datasets** | usually used for small datasets | high performance models and large datasets that require fast execution|\n",
    "\n",
    "This is also a _**non-issue**_ - as you can leverage `tensorflow` commands within `keras` and vice versa. If Keras ever seems slower, it's because the developer's time is more expensive than the GPUs'. Keras is designed with the developer in mind. \n",
    "\n",
    "[reference link](https://www.edureka.co/blog/keras-vs-tensorflow-vs-pytorch/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
