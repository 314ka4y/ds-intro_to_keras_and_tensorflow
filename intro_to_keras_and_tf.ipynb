{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction-to-keras-and-tensorflow\" data-toc-modified-id=\"Introduction-to-keras-and-tensorflow-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction to <code>keras</code> and <code>tensorflow</code></a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#How-Traning-Neural-Networks-Work\" data-toc-modified-id=\"How-Traning-Neural-Networks-Work-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>How Traning Neural Networks Work</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-Propagation\" data-toc-modified-id=\"Forward-Propagation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Forward Propagation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-Propagation-Implemented-from-Scratch\" data-toc-modified-id=\"Forward-Propagation-Implemented-from-Scratch-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Forward Propagation Implemented from Scratch</a></span></li></ul></li><li><span><a href=\"#Backpropagation-(Backprop)\" data-toc-modified-id=\"Backpropagation-(Backprop)-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Backpropagation (Backprop)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Function-Approximation\" data-toc-modified-id=\"Function-Approximation-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>Function Approximation</a></span></li><li><span><a href=\"#Loss-Function\" data-toc-modified-id=\"Loss-Function-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>Loss Function</a></span></li></ul></li><li><span><a href=\"#Adjusting-Weights-with-Gradient-Descent\" data-toc-modified-id=\"Adjusting-Weights-with-Gradient-Descent-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Adjusting Weights with Gradient Descent</a></span></li></ul></li><li><span><a href=\"#Tensorflow-and-Keras\" data-toc-modified-id=\"Tensorflow-and-Keras-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Tensorflow and Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras-is-an-API\" data-toc-modified-id=\"Keras-is-an-API-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Keras is an API</a></span></li><li><span><a href=\"#Creating-and-Using-a-Neural-Network\" data-toc-modified-id=\"Creating-and-Using-a-Neural-Network-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Creating and Using a Neural Network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialize-a-Linear-Stack-of-Layers\" data-toc-modified-id=\"Initialize-a-Linear-Stack-of-Layers-4.2.1\"><span class=\"toc-item-num\">4.2.1&nbsp;&nbsp;</span>Initialize a Linear Stack of Layers</a></span></li><li><span><a href=\"#Add-Densely-Connected-Layers\" data-toc-modified-id=\"Add-Densely-Connected-Layers-4.2.2\"><span class=\"toc-item-num\">4.2.2&nbsp;&nbsp;</span>Add Densely Connected Layers</a></span></li><li><span><a href=\"#Compile-the-Model\" data-toc-modified-id=\"Compile-the-Model-4.2.3\"><span class=\"toc-item-num\">4.2.3&nbsp;&nbsp;</span>Compile the Model</a></span></li><li><span><a href=\"#Investigate-the-Model\" data-toc-modified-id=\"Investigate-the-Model-4.2.4\"><span class=\"toc-item-num\">4.2.4&nbsp;&nbsp;</span>Investigate the Model</a></span></li><li><span><a href=\"#Fit-the-Model\" data-toc-modified-id=\"Fit-the-Model-4.2.5\"><span class=\"toc-item-num\">4.2.5&nbsp;&nbsp;</span>Fit the Model</a></span></li><li><span><a href=\"#Evaluating-the-Trained-Model\" data-toc-modified-id=\"Evaluating-the-Trained-Model-4.2.6\"><span class=\"toc-item-num\">4.2.6&nbsp;&nbsp;</span>Evaluating the Trained Model</a></span></li></ul></li><li><span><a href=\"#Saving-for-Later\" data-toc-modified-id=\"Saving-for-Later-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Saving for Later</a></span></li><li><span><a href=\"#Appendix:-More-on-Tensorflow-Vs.-Keras\" data-toc-modified-id=\"Appendix:-More-on-Tensorflow-Vs.-Keras-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Appendix: More on Tensorflow Vs. Keras</a></span><ul class=\"toc-item\"><li><span><a href=\"#Let's-start-with-tensors\" data-toc-modified-id=\"Let's-start-with-tensors-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Let's start with tensors</a></span></li><li><span><a href=\"#TensorFlow-manages-the-flow-of-matrix-math\" data-toc-modified-id=\"TensorFlow-manages-the-flow-of-matrix-math-4.4.2\"><span class=\"toc-item-num\">4.4.2&nbsp;&nbsp;</span>TensorFlow manages the flow of matrix math</a></span></li></ul></li><li><span><a href=\"#TensorFlow-has-more-levers-and-buttons,-but-Keras-is-more-user-friendly\" data-toc-modified-id=\"TensorFlow-has-more-levers-and-buttons,-but-Keras-is-more-user-friendly-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>TensorFlow has more levers and buttons, but Keras is more user-friendly</a></span><ul class=\"toc-item\"><li><span><a href=\"#Keras,-an-API-with-an-intentional-UX\" data-toc-modified-id=\"Keras,-an-API-with-an-intentional-UX-4.5.1\"><span class=\"toc-item-num\">4.5.1&nbsp;&nbsp;</span>Keras, an API with an intentional UX</a></span></li><li><span><a href=\"#A-few-comparisons\" data-toc-modified-id=\"A-few-comparisons-4.5.2\"><span class=\"toc-item-num\">4.5.2&nbsp;&nbsp;</span>A few comparisons</a></span></li></ul></li></ul></li><li><span><a href=\"#â™«-Better,-Faster,-Stronger-â™«\" data-toc-modified-id=\"â™«-Better,-Faster,-Stronger-â™«-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>â™« Better, Faster, Stronger â™«</a></span><ul class=\"toc-item\"><li><span><a href=\"#Some-useful-resources-on-optimization\" data-toc-modified-id=\"Some-useful-resources-on-optimization-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Some useful resources on optimization</a></span></li><li><span><a href=\"#A-Need-for-Optimization:-Vanishing-Gradient-Problem\" data-toc-modified-id=\"A-Need-for-Optimization:-Vanishing-Gradient-Problem-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>A Need for Optimization: Vanishing Gradient Problem</a></span></li><li><span><a href=\"#Getting-Stuck-in-Rut\" data-toc-modified-id=\"Getting-Stuck-in-Rut-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Getting Stuck in Rut</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Starts\" data-toc-modified-id=\"Random-Starts-5.3.1\"><span class=\"toc-item-num\">5.3.1&nbsp;&nbsp;</span>Random Starts</a></span></li><li><span><a href=\"#Momentum\" data-toc-modified-id=\"Momentum-5.3.2\"><span class=\"toc-item-num\">5.3.2&nbsp;&nbsp;</span>Momentum</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution?--Give-it-some-speed-ðŸ˜Ž\" data-toc-modified-id=\"Solution?--Give-it-some-speed-ðŸ˜Ž-5.3.2.1\"><span class=\"toc-item-num\">5.3.2.1&nbsp;&nbsp;</span>Solution?  Give it some speed ðŸ˜Ž</a></span></li><li><span><a href=\"#Nesterov-Momentum\" data-toc-modified-id=\"Nesterov-Momentum-5.3.2.2\"><span class=\"toc-item-num\">5.3.2.2&nbsp;&nbsp;</span>Nesterov Momentum</a></span></li></ul></li></ul></li><li><span><a href=\"#Batch-Gradient-Descent\" data-toc-modified-id=\"Batch-Gradient-Descent-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Batch Gradient Descent</a></span></li><li><span><a href=\"#Normalization-(Batch-Normalization)\" data-toc-modified-id=\"Normalization-(Batch-Normalization)-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Normalization (Batch Normalization)</a></span></li><li><span><a href=\"#Stochastic-Gradient-Descent\" data-toc-modified-id=\"Stochastic-Gradient-Descent-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Stochastic Gradient Descent</a></span><ul class=\"toc-item\"><li><span><a href=\"#Steps\" data-toc-modified-id=\"Steps-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>Steps</a></span></li><li><span><a href=\"#Mini-Batch-Gradient-Descent\" data-toc-modified-id=\"Mini-Batch-Gradient-Descent-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>Mini-Batch Gradient Descent</a></span></li></ul></li><li><span><a href=\"#Activation-Functions\" data-toc-modified-id=\"Activation-Functions-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Activation Functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Recall-What-an-Activation-Function-Does\" data-toc-modified-id=\"Recall-What-an-Activation-Function-Does-5.7.1\"><span class=\"toc-item-num\">5.7.1&nbsp;&nbsp;</span>Recall What an Activation Function Does</a></span></li><li><span><a href=\"#Using-the-Proper-Activation\" data-toc-modified-id=\"Using-the-Proper-Activation-5.7.2\"><span class=\"toc-item-num\">5.7.2&nbsp;&nbsp;</span>Using the Proper Activation</a></span></li></ul></li><li><span><a href=\"#Aside:-Optimizers\" data-toc-modified-id=\"Aside:-Optimizers-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;</span>Aside: Optimizers</a></span><ul class=\"toc-item\"><li><span><a href=\"#AdaGrad-&amp;-RMSProp\" data-toc-modified-id=\"AdaGrad-&amp;-RMSProp-5.8.1\"><span class=\"toc-item-num\">5.8.1&nbsp;&nbsp;</span>AdaGrad &amp; RMSProp</a></span></li><li><span><a href=\"#Adam-(Adaptive-Moment-Estimation)-Optimization-and-Other-Variants\" data-toc-modified-id=\"Adam-(Adaptive-Moment-Estimation)-Optimization-and-Other-Variants-5.8.2\"><span class=\"toc-item-num\">5.8.2&nbsp;&nbsp;</span>Adam (Adaptive Moment Estimation) Optimization and Other Variants</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction to `keras` and `tensorflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.datasets import load_digits, load_sample_images\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Describe the concept of backpropagation\n",
    "- Explain the use of gradient descent in neural networks\n",
    "- Use `keras` to code up a neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# How Traning Neural Networks Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![backprop](img/ff-bb.gif)\n",
    "\n",
    "The graphic above can be a bit frustrating since it moves fast, but follow the progress as so:\n",
    "\n",
    "Forward propagation with the **blue** tinted arrows computes the output of each layer: i.e. a summation and activation.\n",
    "\n",
    "Backprop calculates the partial derivative (**green** circles) for each weight (**brown** line) and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we saw from last time, forward propagation. Is making a prediction with the neural network after the weights are \"frozen\" (unchanging). \n",
    "\n",
    "We feed in each data point through the neural network to make a prediction. This is usually relatively quick and the real magic comes in when we try to find ways to adjust our different weights to get a better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Forward Propagation Implemented from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's consider some images that we'll use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "flat_image = np.array(digits.data[0]).reshape(digits.data[0].shape[0], -1)\n",
    "eight_by_eight_image = digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eight_by_eight_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(eight_by_eight_image);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(digits.data,\n",
    "                                                    digits.target,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Z is the input from our collector, the sum of the weights\n",
    "# multiplied by the features and the bias\n",
    "\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Input the sum of our weights times the pixel intensities, plus the bias\n",
    "    Output a number between 0 and 1.\n",
    "    \n",
    "    '''\n",
    "    return 1/(1+np.e**(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Simulation from last time\n",
    "\n",
    "w_1 = np.random.normal(0, 0.1, (X_train.shape[1], 1))\n",
    "w_1.shape\n",
    "\n",
    "b_1 = 0\n",
    "\n",
    "z_1 = X_train.dot(w_1) + b_1\n",
    "z_1\n",
    "\n",
    "output = sigmoid(z_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = output > .5\n",
    "y_hat = y_pred.astype(int)\n",
    "y_hat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Backpropagation (Backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After a certain number of data points have been passed through the model, the weights will be *updated* with an eye toward optimizing our loss function. (Thinking back to biological neurons, this is like revising their activation potentials.) Typically, this is  done  by using some version of gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![bprop](img/BackProp_web.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Neural networks are much like computational graphs](https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9).\n",
    "\n",
    "And computational graphs can be used [to approximate *any* function](http://neuralnetworksanddeeplearning.com/chap4.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we're thinking of networks, then, as function approximators, of course we'll want to know how good the approximation is. And so once again we have the idea of a [loss function](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html), which is of course what licenses our thinking of networks as models in the first place.\n",
    "\n",
    "Many loss functions are available. Your choice will depend in part on whether you're doing a regression or classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Regression:\n",
    "\n",
    "- mean / median absolute error\n",
    "- mean / median squared error\n",
    "- [Huber loss](https://en.wikipedia.org/wiki/Huber_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Classification:\n",
    "\n",
    "- Crossentropy\n",
    "- [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss)\n",
    "- [Kullback-Leibler divergence](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The loss function tells us how well our model performed by comparing the predictions to the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we train our models with `keras`, we will watch the loss function's progress across epochs.  A decreasing loss function will show us that our model is **improving**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The loss function is associated with the nature of our output. In logistic regression, our output was binary, so our loss function was the negative loglikelihood, aka **cross-entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\Large -\\ loglikelihood = -\\frac{1}{m} * \\sum\\limits_{i=1}^m y_i\\log{p_i} + (1-y_i)\\log(1-p_i) $$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train %2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_train*np.log(output) + (1-y_train) * np.log(1-output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "neg_ll = -1/len(y_train)*np.sum(y_train*np.log(output) + (1-y_train) * np.log(1-output))\n",
    "neg_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For continuous variables, the loss function we have relied on is [MSE or MAE](http://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/).\n",
    "\n",
    "Good [resource](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) on backpropogation with RMSE loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is a good summary of different [loss functions]( https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Adjusting Weights with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We not only use the the loss function to see how our model is improving; we also use it to update our parameters. The gradient of the loss function is calculated in relation to each parameter of our neural net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For a deep dive into the fitting process, reference Chapter 11 in [Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Gradient descent can be performed in several different ways.  Unlike the `sklearn` implementation of linear regression, which finds the minimum of the loss with a closed form solution, neural networks move down the gradient **incrementally**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When **all examples** from a training set have passed through the network and weights are adjusted with backpropagation, we say an ***epoch*** has been completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Wait a second, what is that warning? \n",
    "`Using TensorFlow backend.`\n",
    "\n",
    "<img align =left src=\"img/keras.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Keras is an API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It can be layered on top of many different back-end processing systems.\n",
    "\n",
    "![kerasback](img/keras_tf_theano.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While each of these systems has its own coding methods, `keras` abstracts from that in the streamlined Pythonic manner we are used to seeing in other Python modeling libraries.\n",
    "\n",
    "Keras development is backed primarily by Google, and the Keras API comes packaged in TensorFlow as tf.keras. Additionally, Microsoft maintains the CNTK Keras backend. Amazon AWS is maintaining the Keras fork with MXNet support. Other contributing companies include NVIDIA, Uber, and Apple (with CoreML).\n",
    "\n",
    "Theano has been discontinued.  The last release was 2017, but can still be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use TensorFlow, as it is the most popular. TensorFlow became the most used Keras backend, and  eventually integrated Keras in via its `tf.keras` submodule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Creating and Using a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will start with a binary classification, and predict whether the number will be even or odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_binary = y % 2\n",
    "y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Initialize a Linear Stack of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Sequential` is referring to the neural networks we've observed. There are other neural network models that will go beyond this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Add Densely Connected Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The actual network; we can decide how many layers & nodes for each layer here as well as other hyperparameters like the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=30, activation='relu', input_dim=64, name='First_Layer'))\n",
    "# Use a 2nd hidden layer for more parameters & complexity\n",
    "# model.add(Dense(units=16, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The next step is new: After building the model we'll now **compile** it, which is a matter of yoking together the architecture with:\n",
    "- the optimizer we want to use,\n",
    "- the loss function we want to use, and\n",
    "- the metrics we want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# A simple model with little optimizers\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Aside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Alterntative model to compile\n",
    "optimize_model = Sequential()\n",
    "optimize_model.add(Dense(units=30, activation='relu', input_dim=64))\n",
    "optimize_model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Using optimizers\n",
    "from keras import optimizers\n",
    "optimize_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Investigate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.get_layer('First_Layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "weights, biases = model.layers[0].get_weights()\n",
    "\n",
    "print('Weights:\\n', weights)\n",
    "print()\n",
    "print('Biases:\\n', biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Using the model structure, we do sequences of feed forward and then backpropagation to adjust the weights and biases (training/fitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we're ready to **fit** it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving history to refer to\n",
    "history = model.fit(X, y_binary, epochs=50, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# alternatively could have specified a specific batch to train on\n",
    "# model.train_on_batch(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's take a look at the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# See how the weights and biases changed\n",
    "weights, biases = model.layers[0].get_weights()\n",
    "\n",
    "print('Weights:\\n', weights)\n",
    "print()\n",
    "print('Biases:\\n', biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Evaluating the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(history.history)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can look at the overall loss from our test data after training the model was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Could also just use a batch to evaluate\n",
    "# loss_and_metrics = model.evaluate(x_test, y_test, batch_size=16)\n",
    "\n",
    "loss_and_metrics = model.evaluate(X, y)\n",
    "\n",
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can have predictions (probability the data point is a particular class based on our trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classes = model.predict(X)\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We want to say what is the predicted class, so we pick just the largest probability for each result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions = np.argmax(classes, axis=1)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we can see how accurate our model was by seeing if the predicted classes match the actual labels. Note that this is calculated differently from how the loss is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.sum(predictions  == np.argmax(X, axis=1)) / predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Saving for Later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Appendix: More on Tensorflow Vs. Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Let's start with tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Tensors are multidimensional matrices.\n",
    "\n",
    "![tensor](img/tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### TensorFlow manages the flow of matrix math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That makes neural network processing possible.\n",
    "\n",
    "![cat](img/cat-tensors.gif)\n",
    "\n",
    "For our numbers dataset, our tensors from the `sklearn` dataset were originally tensors of the shape 8x8, i.e. 64-bit pictures. Remember, that was with black and white images.\n",
    "\n",
    "For image processing, we are often dealing with color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image = load_sample_images()['images'][0]\n",
    "\n",
    "imgplot = plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What do the dimensions of our image above represent?\n",
    "\n",
    "Tensors with higher numbers of dimensions have a higher **rank**.\n",
    "\n",
    "A matrix with rows and columns only, like the black and white numbers, has **rank 2**.\n",
    "\n",
    "A matrix with a third dimension, like the color pictures above, has **rank 3**.\n",
    "\n",
    "When we flatten an image by stacking the rows in a column, we are decreasing the rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "flat_image = image.reshape(-1, 1)\n",
    "\n",
    "flat_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "427*640*3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## TensorFlow has more levers and buttons, but Keras is more user-friendly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Coding directly in **Tensorflow** allows you to tweak more parameters to optimize performance. The **Keras** wrapper makes the code more accessible for developers prototyping models.\n",
    "\n",
    "![levers](img/levers.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Keras, an API with an intentional UX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Deliberately design end-to-end user workflows\n",
    "- Reduce cognitive load for your users\n",
    "- Provide helpful feedback to your users\n",
    "\n",
    "[full article here](https://blog.keras.io/user-experience-design-for-apis.html)<br>\n",
    "[full list of why to use Keras](https://keras.io/why-use-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### A few comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While you **can leverage both**, here are a few comparisons.\n",
    "\n",
    "| Comparison | Keras | Tensorflow|\n",
    "|------------|-------|-----------|\n",
    "| **Level of API** | high-level API | High and low-level APIs |\n",
    "| **Speed** |  can *seem* slower |  is a bit faster |\n",
    "| **Language architecture** | simple architecture, more readable and concise | straight tensorflow is a bit more complex |\n",
    "| **Debugging** | less frequent need to debug | difficult to debug |\n",
    "| **Datasets** | usually used for small datasets | high performance models and large datasets that require fast execution|\n",
    "\n",
    "This is also a _**non-issue**_ - as you can leverage `tensorflow` commands within `keras` and vice versa. If Keras ever seems slower, it's because the developer's time is more expensive than the GPUs'. Keras is designed with the developer in mind. \n",
    "\n",
    "[reference link](https://www.edureka.co/blog/keras-vs-tensorflow-vs-pytorch/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# â™« Better, Faster, Stronger â™«"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can optimize our model to run faster and better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes this optimization feels like black magic over a science...\n",
    "\n",
    "![https://xkcd.com/1838/](https://imgs.xkcd.com/comics/machine_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Some useful resources on optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Keras documentation & discussion: https://keras.io/optimizers/\n",
    "- Excellent blog post on optimizations: http://ruder.io/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## A Need for Optimization: Vanishing Gradient Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When our gradients are too small we can run into a few issues when training our model (adjusting weights and biases):\n",
    "- Slow training\n",
    "- Local minimum problem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Getting Stuck in Rut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Random Starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To avoid local minimums, what if we just randomly initialize ourselves somewhere on the cost function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll have to run the same model multiple times, but we start off with different weights and can help avoid falling into a local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Recall gradient descent is analogous to finding the lowest point of the cost function ($J$) by adjusting the weights $W$ by the gradient of the cost function (multiplied by the learning factor)\n",
    "\n",
    "$$ W \\leftarrow W - \\alpha \\nabla J(W) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/cartoon_descender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But sometimes we can get \"trapped\" in a **local minimum**, a spot that is low compared to the nearby area but not the lowest point overall. Think of a ball rolling down the hill getting stuck at a plateau or slight incline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Solution?  Give it some speed ðŸ˜Ž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can \"power\" over the hill with **momentum**; the gradient term of our weights update will be our _acceleration_:\n",
    "\n",
    "$$ V_m \\leftarrow \\beta V_m - \\alpha \\nabla J(W) $$\n",
    "$$ W \\leftarrow W + V_m $$\n",
    "\n",
    "where $\\beta$ is our momentum hyperparameter (usually set to about 0.9)\n",
    "\n",
    "In addition to going up the hill, it also can speed up gradient descent! Note that we can also suffer from oscillating around the minimum before settling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One solution to optimize further is using the Nesterov momentum optimization (also referenced as NAG for \"Nesterov Accelerated Gradient\"). This basically does momentum like before but considers the gradient ahead of where it actually is.\n",
    "\n",
    "$$ V_m \\leftarrow \\beta V_m - \\alpha \\nabla J(W+\\beta V_m) $$\n",
    "$$ W \\leftarrow W + V_m $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "import keras\n",
    "\n",
    "sgd_optimizer = keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=sgd_optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The gradient is calculated across all values.  We can find the direction of the gradient, and proceed directly towards the minimum.\n",
    "\n",
    "The weights are updated with regard to the cost at the **end of an epoch** after all training elements have passed through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Normalization (Batch Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> https://keras.io/layers/normalization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This allows speedier training so no one feature will overpower the direction down the hill\n",
    "\n",
    "Note if you use something like batch normalization, training speed can slow but converging to the minimal cost will tend to happen sooner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "SGD updates the weights after each training **example**. SGD requires fewer epochs to achieve quality coefficients. This speeds up gradient descent [significantly](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Instead of taking a long time through the whole process with dataset (careful step), go through part of the dataset quicker (quick, \"drunken\" steps). Note that this means it will also take less space in memory.\n",
    "\n",
    "\"Stochastic\" == Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/sgd_visualization.png)\n",
    "\n",
    "> Note: You likely will want to combine this momentum and other tweaks like learning rate decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Take a random set (batch)\n",
    "2. Feed forward batch through model\n",
    "3. Calculate error (loss) from batch\n",
    "4. Adjust weights via backpropogagtion\n",
    "5. Repeat with all points/batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In mini-batch, we pass a batch, calculate the gradient, update the params, then proceed to the next batch. It combines the advantages of batch and stochastic gradient descent: it is faster than SGD since the updates are not made with each point, and more computationally efficient than batch, since we don't have to hold all training examples in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "[Good comparison of types of Gradient Descent and batch size](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> We can modify what activation we use to help with the gradient during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Recall What an Activation Function Does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/relu_bouncer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using the Proper Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A simple way to change the gradient, is change the activation function that produces different gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/activations_ranked.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In general:\n",
    "\n",
    "* Sigmoid should almost never be used anymore for an activation \n",
    "* ReLU is a good balance of speed and performance (most systems have optimizations)\n",
    "* You may find other variants like leaky ReLU to give better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Aside: Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Different types of gradient descent update the parameters at different times in different ways.\n",
    "\n",
    "> https://keras.io/optimizers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One of the levers we can tweak are the optimizers which control how the weights and biases are updated.\n",
    "\n",
    "For stochastic gradient descent, the weights are updated with a **constant** learning rate (alpha) after every record.  If we specify a batch size, the constant learning rate is multiplied by the gradient across the batch. \n",
    "\n",
    "Other optimizers, such as **Adam** (\"Adaptive Moment Estimation\"), update the weights in different ways. For Adam,\n",
    "> A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds. See [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### AdaGrad & RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Quick aside, typically we don't use AdaGrad as it will tend to stop too early in complex problems (neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One problem with gradient descent is we might take too many big jumps on the steep section. **AdaGrad** and the improvement from **RMSProp** allows us to be more careful when it's steep. AdaGrad accumulates avearage gradients from all iterations while RMSProp will decay the older iterations (keeping mostly the most recent iterations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/adagrad_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "import keras\n",
    "# Typically rho=0.9 is a good value\n",
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Adam (Adaptive Moment Estimation) Optimization and Other Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Combines momentum and RMSProp strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# Typical values for Adam\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Nadam**: Basically combines Adam and Nesterov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```python\n",
    "# Typical values for Nadam\n",
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(optimizer=optimizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Note**: Sometimes Adam just isn't good for a dataset; a good alternative is just using plain old momentum with Nesterov"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
